import threading
import os
import pandas as pd
import time
import tweepy
from fetch_tweets import fetch_tweets
from data_loader import load_data
from text_cleaner import clean_dataframe
from feature_generator import generate_features
from calculateKPI import calculateKPI
from analyze_tweets import analyze

def wait_and_retry():
    print("Erreur 429: Trop de requ√™tes. Attente de 15 minutes...")
    time.sleep(15 * 60)  # Attendre 15 minutes
    print("Reprise apr√®s 15 minutes.")
    fetch_tweets("ENGIEpartFR")  # Relancer la r√©cup√©ration des tweets

def fetch_tweets_in_background():
    fetch_tweets("ENGIEpartFR")

def analyze_in_background():
    # Lancer l'analyse des tweets en arri√®re-plan
    try:
        analyze()  # Fonction d'analyse des tweets
    except tweepy.errors.TooManyRequests as e:
        wait_and_retry()  # Attendre et relancer en cas de limite d'API

# Fonction pour mettre √† jour le fichier cleaned_data.csv avec de nouvelles donn√©es
def update_cleaned_data():
    file_path = "./filtered_tweets_engie1.csv"
    result_path = "./result_csv/cleaned_data.csv"

    # Charger les nouvelles donn√©es
    if os.path.exists(file_path):
        try:
            new_data = pd.read_csv(file_path, sep=';', encoding='utf-8')
            print(f"üì• Nouvelles donn√©es r√©cup√©r√©es de {file_path}.")
        except pd.errors.EmptyDataError:
            print(f"‚ö†Ô∏è Le fichier {file_path} est vide ou mal form√©.")
            return
        except Exception as e:
            print(f"‚ùå Erreur lors de la lecture du fichier {file_path} : {e}")
            return
    else:
        print("Le fichier de tweets n'existe pas.")
        return

    # Charger les anciennes donn√©es si elles existent
    if os.path.exists(result_path):
        df_cleaned = pd.read_csv(result_path, sep=';', encoding='utf-8')
        print(f"üìÇ Chargement des anciennes donn√©es de {result_path}.")
    else:
        print(f"Le fichier {result_path} n'existe pas, il va √™tre cr√©√©.")
        df_cleaned = pd.DataFrame()

    # Nettoyer les nouvelles donn√©es
    new_data_cleaned = clean_dataframe(new_data)

    # V√©rification avant concat√©nation
    print(f"Anciennes donn√©es : {df_cleaned.shape[0]} lignes.")
    print(f"Nouvelles donn√©es nettoy√©es : {new_data_cleaned.shape[0]} lignes.")

    # Ajouter les nouvelles donn√©es nettoy√©es au DataFrame existant
    df_combined = pd.concat([df_cleaned, new_data_cleaned], ignore_index=True)

    # Supprimer les doublons en utilisant la colonne 'id'
    df_combined = df_combined.drop_duplicates(subset=['id'])

    # V√©rification apr√®s suppression des doublons
    print(f"Nombre de lignes apr√®s suppression des doublons : {df_combined.shape[0]}.")

    # Sauvegarder les donn√©es nettoy√©es dans le fichier CSV
    df_combined.to_csv(result_path, sep=';', index=False)
    print(f"‚úÖ Les nouvelles donn√©es ont √©t√© ajout√©es au fichier {result_path}")


def main():
    result_path = "./result_csv/cleaned_data.csv"
    file_path = "./filtered_tweets_engie1.csv"

    # V√©rifier si le fichier de r√©sultats existe d√©j√†
    if os.path.exists(result_path):
        print(f"üìÇ Chargement du fichier existant : {result_path}")
        df = pd.read_csv(result_path, sep=';', encoding='utf-8')
    else:
        print(f"üìÇ Cr√©ation du fichier nettoy√©...")
        # Charger les donn√©es √† partir de filtered_tweets_engie1.csv
        if os.path.exists(file_path):
            df = pd.read_csv(file_path, sep=';', encoding='utf-8')
        else:
            # Si le fichier n'existe pas, on cr√©e un DataFrame vide
            df = pd.DataFrame()

        # Nettoyer et traiter les donn√©es (ajustez les fonctions selon vos besoins)
        df = clean_dataframe(df)
        df = generate_features(df)
        df.to_csv(result_path, sep=';', index=False)

    # Lancer la r√©cup√©ration des tweets en parall√®le
    tweet_thread = threading.Thread(target=fetch_tweets, args=("(@ENGIEpartFR OR @ENGIEpartSAV) -is:retweet lang:fr",))
    tweet_thread.start()

    # Lancer l'analyse des tweets en parall√®le
    analysis_thread = threading.Thread(target=analyze_in_background)
    analysis_thread.start()

    # Mettre √† jour le fichier cleaned_data.csv avec de nouvelles donn√©es (ajouter les tweets r√©cents)
    update_cleaned_data()

    # Calculer les KPI
    calculateKPI()

    # Attendre que le thread fetch_tweets termine
    tweet_thread.join()

    # Attendre que le thread analyse termine
    analysis_thread.join()

if __name__ == "__main__":
    main()